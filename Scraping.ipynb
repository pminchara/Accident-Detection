{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "alWnC8eTWXhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58fe9dd-6292-43e9-bdc8-392e2754e6ec"
      },
      "source": [
        "import os\r\n",
        "import requests \r\n",
        "from bs4 import BeautifulSoup # pip install bs4 #to parse html(getting data out from html, xml or other markup languages)\r\n",
        "#from proxycrawl.proxycrawl_api import ProxyCrawlAPI \r\n",
        "\r\n",
        "# user can input a search keyword and the count of images required\r\n",
        "# download images from google search image\r\n",
        "Google_Image = \\\r\n",
        "    'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\r\n",
        "\r\n",
        "# The User-Agent request header contains a characteristic string \r\n",
        "# that allows the network protocol peers to identify the application type, \r\n",
        "# operating system, and software version of the requesting software user agent.\r\n",
        "# needed for google search\r\n",
        "u_agnt = {\r\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',\r\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\r\n",
        "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\r\n",
        "    'Accept-Encoding': 'none',\r\n",
        "    'Accept-Language': 'en-US,en;q=0.8',\r\n",
        "    'Connection': 'keep-alive',\r\n",
        "} #write: 'my user agent' in browser to get your browser user agent details\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Image_Folder = 'Google_Images'\r\n",
        "\r\n",
        "def main():\r\n",
        "    if not os.path.exists(Image_Folder):\r\n",
        "        os.mkdir(Image_Folder)\r\n",
        "    download_images_from_google()\r\n",
        "\r\n",
        "def download_images_from_google():\r\n",
        "    data = input('Google keyword for Image Search: ')\r\n",
        "    num_images = int(input('Number of Images to be downloaded: '))\r\n",
        "    \r\n",
        "    \r\n",
        "    search_url = Google_Image + 'q=' + data #'q=' because its a query\r\n",
        "    #api = ProxyCrawlAPI({'token': 'Oqm3MrqgRq7rSLM34hArlA'}) # https://proxycrawl.com/ for trail and paid\r\n",
        "    #response = api.get(search_url, {'scroll': 'true', 'scroll_interval': '60', 'ajax_wait': 'true'}) #Parameters of ProxyCrawl\r\n",
        "    #if response['status_code'] == 200:\r\n",
        "    # request url, without u_agnt the permission gets denied\r\n",
        "    response = requests.get(search_url, headers=u_agnt)\r\n",
        "    html = response.text \r\n",
        "    #else :\r\n",
        "     #   print(\"error\")\r\n",
        "    # find all img where class='rg_i Q4LuWd'\r\n",
        "    b_soup = BeautifulSoup(html, 'html.parser') #html.parser is used to parse/extract features from HTML files\r\n",
        "    results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\r\n",
        "    \r\n",
        "    #extract the links of requested number of images with 'data-src' attribute and appended those links to a list 'imagelinks'\r\n",
        "    #allow to continue the loop in case query fails for non-data-src attributes\r\n",
        "    count = 0\r\n",
        "    imagelinks= []\r\n",
        "    for res in results:\r\n",
        "        try:\r\n",
        "            link = res['data-src']\r\n",
        "            imagelinks.append(link)\r\n",
        "            count += 1\r\n",
        "            if (count >= num_images):\r\n",
        "                break\r\n",
        "            \r\n",
        "        except KeyError:\r\n",
        "            continue\r\n",
        "    \r\n",
        "    print(f'Found {len(imagelinks)} images')\r\n",
        "    print('Start Downloading')\r\n",
        "\r\n",
        "    for i, imagelink in enumerate(imagelinks):\r\n",
        "        # open each image link and save the file\r\n",
        "        response = requests.get(imagelink)\r\n",
        "        \r\n",
        "        imagename = Image_Folder + '/' + data + str(i+1) + '.jpg'\r\n",
        "        with open(imagename, 'wb') as file:\r\n",
        "            file.write(response.content)\r\n",
        "    print('Download Completed!')\r\n",
        "    \r\n",
        "main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google keyword for Image Search: car\n",
            "Number of Images to be downloaded: 2\n",
            "Found 2 images\n",
            "Start Downloading\n",
            "Download Completed!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}